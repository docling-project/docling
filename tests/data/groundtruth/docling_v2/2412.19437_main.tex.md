UTF8gbsn

Image: figures/dsv3\_performance.pdf

<!-- image -->

Benchmark performance of DeepSeek-V3 and its counterparts.

0.9

## Introduction

In recent years, Large Language Models

(LLMs) have been undergoing rapid iteration and evolution[gpt4o,claude35sonnet,gemini1\_5], progressively diminishing the gap towards Artificial General Intelligence(AGI).
Beyond closed-source models, open-source models, including DeepSeek series[dsvi,dsvii,dscodervi,dscodervii], LLaMA series[llama,llama2,llama3,llama3\_1\_405b], Qwen series[qwen,qwen1\_5,qwen2\_5], and Mistral series[mistral,mixtral8x22b], are also making significant strides, endeavoring to close the gap with their closed-source counterparts.
To further push the boundaries of open-source model capabilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts

(MoE) model with 671B parameters, of which 37B are activated for each token.

With a forward-looking perspective, we consistently strive for strong model performance and economical costs.
Therefore, in terms of architecture,

DeepSeek-V3 still adopts Multi-head Latent Attention(MLA)[dsvii] for efficient inference and DeepSeekMoE[deepseekmoe] for cost-effective training. 
These two architectures have been validated in DeepSeek-V2[dsvii], demonstrating their capability to maintain robust model performance while achieving efficient training and inference.
Beyond the basic architecture, we implement two additional strategies to further enhance the model capabilities. 
Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy[noaux\_tc] for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing.
Secondly, DeepSeek-V3

employs a multi-token prediction training objective, which we have observed to enhance the overall performance on evaluation benchmarks.

In order to achieve efficient training, we support the FP8 mixed precision training and implement comprehensive optimizations for the training framework.
Low-precision training has emerged as a promising solution for efficient training

[bf16train, fp16train, fp8lm, llm.int8], its evolution being closely tied to advancements in hardware capabilities[fp8format, hifp8format, microscaling]. 
In this work, we introduce an FP8 mixed precision training framework and, for the first time, validate its effectiveness on an extremely large-scale model. 
Through the support for FP8 computation and storage, we achieve both accelerated training and reduced GPU memory usage. 
As for the training framework, we design the DualPipe algorithm for efficient pipeline parallelism, which has fewer pipeline bubbles and hides most of the communication during training through computation-communication overlap.
This overlap ensures that, as the model further scales up, as long as we maintain a constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead.
In addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize InfiniBand(IB) and NVLink bandwidths. 
Furthermore, we meticulously optimize the memory footprint, making it possible to train DeepSeek-V3

without using costly tensor parallelism.
Combining these efforts, we achieve high training efficiency.

During pre-training, we train

DeepSeek-V3 on 14.8T high-quality and diverse tokens. 
The pre-training process is remarkably stable. 
Throughout the entire training process, we did not encounter any irrecoverable loss spikes or have to roll back.
Next, we conduct a two-stage context length extension for DeepSeek-V3. In the first stage, the maximum context length is extended to 32K, and in the second stage, it is further extended to 128K. 
Following this, we conduct post-training, including Supervised Fine-Tuning(SFT) and Reinforcement Learning(RL) on the base model of DeepSeek-V3

, to align it with human preferences and further unlock its potential. 
During the post-training stage, we distill the reasoning capability from the DeepSeek-R1 series of models, and meanwhile carefully maintain the balance between model accuracy and generation length.

We evaluate

DeepSeek-V3 on a comprehensive array of benchmarks. 
Despite its economical training costs, comprehensive evaluations reveal that DeepSeek-V3

-Base has emerged as the strongest open-source base model currently available, especially in code and math. 
Its chat version also outperforms other open-source models and achieves performance comparable to leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard and open-ended benchmarks.

6pt