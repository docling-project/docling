-30pt

Image: images/header.jpeg

<!-- image -->

## Abstract

We introduce , a 7billion-parameter language model engineered for superior performance and efficiency.
outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation.
Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost.
We also provide a model fine-tuned to follow instructions, , that surpasses 213Bchat model both on human and automated benchmarks.
Our models are released under the Apache 2.0 license.
Code: https://github.com/mistralai/mistral-src 
Webpage: https://mistral.ai/news/announcing-mistral-7b/

## Introduction

=-1 In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model performance often necessitates an escalation in model size.
However, this scaling tends to increase computational costs and inference latency, thereby raising barriers to deployment in practical, real-world scenarios.
In this context, the search for balanced models delivering both high-level performance and efficiency becomes critically essential.
Our model, , demonstrates that a carefully designed language model can deliver high performance while maintaining an efficient inference.
outperforms the previous best 13B model (Llama 2, [touvron2023llama2]) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B,[touvron2023llama]) in mathematics and code generation.
Furthermore, approaches the coding performance of Code-7B[roziere2023code]

, without sacrificing performance on non-code related benchmarks.

leverages grouped-query attention (GQA)[ainslie2023gqa], and sliding window attention (SWA)[child2019generating,beltagy2020longformer]. GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications.
In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of

.

is released under the Apache 2.0 license.
This release is accompanied by a reference implementation

https://github.com/mistralai/mistral-src

facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM[kwon2023efficient] inference server and SkyPilot

https://github.com/skypilot-org/skypilot

.
Integration with Hugging Face

https://huggingface.co/mistralai

is also streamlined for easier integration.
Moreover, is crafted for ease of fine-tuning across a myriad of tasks.
As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from that significantly outperforms the 2 13B

Chat model.

=-1

takes a significant step in balancing the goals of getting high performance while keeping large language models efficient.
Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.

## Architectural details

Image: images/swa.pdf

<!-- image -->

Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence length, and the memory increases linearly with the number of tokens. At inference time, this incurs higher latency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window attention: each token can attend to at most $W$ tokens from the previous layer (here, $W=3$). Note that tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by $W$ tokens. Hence, after $k$ attention layers, information can move forward by up to $k \times W$ tokens.

r0.275
-15pt