item-0 at level 0: unspecified: group _root_
  item-1 at level 1: text: UTF8gbsn
  item-2 at level 1: section: group figure
    item-3 at level 2: picture
      item-3 at level 3: caption: Image: figures/dsv3_performance.pdf
    item-4 at level 2: text: Benchmark performance of DeepSeek-V3 and its counterparts.
  item-5 at level 1: caption: Image: figures/dsv3_performance.pdf
  item-6 at level 1: text: 0.9
  item-7 at level 1: section_header: Introduction
  item-8 at level 1: paragraph: In recent years, Large Language Models
  item-9 at level 1: text: (LLMs) have been undergoing rapi ... eepSeek-V3, a large Mixture-of-Experts
  item-10 at level 1: paragraph: (MoE) model with 671B parameters ... hich 37B are activated for each token.
  item-11 at level 1: paragraph: With a forward-looking perspecti ... .
Therefore, in terms of architecture,
  item-12 at level 1: text: DeepSeek-V3 still adopts Multi-h ...  load balancing.
Secondly, DeepSeek-V3
  item-13 at level 1: paragraph: employs a multi-token prediction ...  performance on evaluation benchmarks.
  item-14 at level 1: paragraph: In order to achieve efficient tr ... mising solution for efficient training
  item-15 at level 1: text: [bf16train, fp16train, fp8lm, ll ... aking it possible to train DeepSeek-V3
  item-16 at level 1: paragraph: without using costly tensor para ... , we achieve high training efficiency.
  item-17 at level 1: paragraph: During pre-training, we train
  item-18 at level 1: text: DeepSeek-V3 on 14.8T high-qualit ... g(RL) on the base model of DeepSeek-V3
  item-19 at level 1: paragraph: , to align it with human prefere ...  model accuracy and generation length.
  item-20 at level 1: paragraph: We evaluate
  item-21 at level 1: text: DeepSeek-V3 on a comprehensive a ... ve evaluations reveal that DeepSeek-V3
  item-22 at level 1: paragraph: -Base has emerged as the stronge ... of standard and open-ended benchmarks.
  item-23 at level 1: text: 6pt